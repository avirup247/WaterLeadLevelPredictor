{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DPC++ with MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objective\n",
    "\n",
    "* Use DPC++ along with MPI on compute clusters with GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Multi-Node Workflow with oneAPI\n",
    "With oneAPI, Accelerated code can be written in either a kernel (DPC++) or __directive based style__. Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA*__ to __Data Parallel C++__. Existing __Fortran__ programmers can leverage __OpenMP*__ for directive-based offload. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__ and existing __OpenCL*__ applications can remain in the OpenCL language or migrate to Data Parallel C++.\n",
    "\n",
    "To take advantage of multi-node clusters with GPUs, the aforementioned standards can be used in conjuction with MPI. The Intel® MPI Library is included with the Intel® oneAPI HPC toolkit.\n",
    "\n",
    "__Intel® Advisor__ is recommended to  __optimize__ the existing design for __vectorization and memory usage__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__ The __Intel® VTune™ Profiler__ can be used to profile your accelerated program while The __Intel® Trace Analyzer and Collector__ can be used to help you understand MPI application behavior.\n",
    "\n",
    "The figure below shows the recommended approach of different starting points for HPC developers.\n",
    "\n",
    "<img src=\"Assets/OneAPI_flow_mpi.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and DPC++\n",
    "\n",
    "The following diagram illustrates how MPI and DPC++ interacts with the various nodes and accelerator devices in a cluster. Currently, the Intel® MPI Library supports host-based MPI where MPI calls are made from the hosts' CPUs. In the future, GPU-aware MPI will be supported so that GPU code will be able to directly interact with MPI.\n",
    "\n",
    "<img src=\"Assets/DPCPP_MPI.JPG\">\n",
    "\n",
    "In the current paradigm, the two standards serve different purposes and can be effectively used together. MPI is used to communicate between nodes, while DPC++ is used to accelerate computation on a single node using the accelerators available, which can include CPUs, GPUs, and FPGAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Pi\n",
    "\n",
    "To illustrate this concept, we're going to use both MPI and DPC++ to calculate pi through numeric integration.\n",
    "\n",
    "$$\n",
    "\\pi=\\int_0^1 \\frac{4}{1+x^2} dx\n",
    "$$\n",
    "\n",
    "The algorithm will use a discretization of the above equation.\n",
    "\n",
    "$$\n",
    "\\pi \\approx \\sum_{n=0}^{N-1} \\frac{4}{1+x_{i}^2} dx\n",
    "$$\n",
    "\n",
    "Where N=Number of steps  or iterations, dx = 1/N, and $x_{i}= (i+0.5)/N$\n",
    "\n",
    "In the code below, in _main()_, the execution of the iterations are divided among MPI processes. Then, for each process, the group of iterations would execute in parallel on the default DPC++ device.\n",
    "\n",
    "Execute the cell below to write the code to file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/pi_mpi_dpcpp.cpp\n",
    "\n",
    "//==============================================================\n",
    "// Copyright © 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "//\n",
    "// PI_MPI_DPCPP: Using Intel® oneAPI DPC++ Language in MPI program.\n",
    "//\n",
    "// Using Data Parallel C++, the code sample runs multiple MPI ranks to\n",
    "// distribute the calculation of the number Pi. Each rank offloads the\n",
    "// computation to an accelerator (GPU/CPU) using Intel DPC++ compiler to compute\n",
    "// a partial compution of the number of Pi.\n",
    "//\n",
    "// For comprehensive instructions regarding DPC++ Programming, go to\n",
    "// https://software.intel.com/en-us/oneapi-programming-guide\n",
    "// and search based on relevant terms noted in the comments.\n",
    "//\n",
    "// DPC++ material used in this code sample:\n",
    "//\n",
    "// Basic structures of DPC++:\n",
    "//   DPC++ Queues (including device selectors and exception handlers)\n",
    "//   DPC++ Buffers and accessors (communicate data between the host and the\n",
    "//   device)\n",
    "//   DPC++ Kernels (including parallel_for function and range<1> objects)\n",
    "//\n",
    "//******************************************************************************\n",
    "// Content: (version 1.0)\n",
    "//      Calculate the number Pi in parallel using its integral representation.\n",
    "//\n",
    "//******************************************************************************\n",
    "#include <mpi.h>\n",
    "#include <CL/sycl.hpp>\n",
    "#include <CL/sycl/intel/fpga_extensions.hpp>\n",
    "\n",
    "using namespace std;\n",
    "using namespace cl::sycl;\n",
    "\n",
    "constexpr int kMaster = 0;\n",
    "constexpr long kIteration = 1024;\n",
    "constexpr long kScale = 45;\n",
    "constexpr long kTotalNumStep = kIteration * kScale;\n",
    "constexpr access::mode sycl_read = access::mode::read;\n",
    "constexpr access::mode sycl_write = access::mode::write;\n",
    "\n",
    "//******************************************************************************\n",
    "// Function description: computes the number Pi partially in parallel using DPC++.\n",
    "// Each MPI rank calls this function to computes the number Pi partially.\n",
    "//******************************************************************************\n",
    "void CalculatePiParallel(float* results, int rank_num, int num_procs);\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "    int i, id, num_procs;\n",
    "    float total_pi;\n",
    "    MPI_Status stat;\n",
    "    \n",
    "    // Start MPI.\n",
    "    if (MPI_Init(&argc, &argv) != MPI_SUCCESS) {\n",
    "        cout << \"Failed to initialize MPI\\n\";\n",
    "        exit(-1);\n",
    "    }\n",
    "\n",
    "    // Create the communicator, and retrieve the number of processes.\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
    "\n",
    "    // Determine the rank of the process.\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
    "\n",
    "    int num_step_per_rank = kTotalNumStep / num_procs;\n",
    "    float* results_per_rank = new float[num_step_per_rank];\n",
    "\n",
    "    for (size_t i = 0; i < num_step_per_rank; i++) results_per_rank[i] = 0.0;\n",
    "\n",
    "    // Calculate the Pi number partially in parallel.\n",
    "    CalculatePiParallel(results_per_rank, id, num_procs);\n",
    "\n",
    "    float sum = 0.0;\n",
    "\n",
    "    for (size_t i = 0; i < num_step_per_rank; i++) sum += results_per_rank[i];\n",
    "\n",
    "    delete[] results_per_rank;\n",
    "\n",
    "    MPI_Reduce(&sum, &total_pi, 1, MPI_FLOAT, MPI_SUM, kMaster, MPI_COMM_WORLD);\n",
    "\n",
    "    if (id == kMaster) cout << \"---> pi= \" << total_pi << \"\\n\";\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////\n",
    "//\n",
    "// Compute the number Pi partially in DPC++ on device: the partial result is\n",
    "// returned in \"results\".\n",
    "//\n",
    "////////////////////////////////////////////////////////////////////////\n",
    "void CalculatePiParallel(float* results, int rank_num, int num_procs) {\n",
    "    char machine_name[MPI_MAX_PROCESSOR_NAME];\n",
    "    int name_len;\n",
    "    int num_step = kTotalNumStep / num_procs;\n",
    "    float* x_pos_per_rank = new float[num_step];\n",
    "    float dx, dx_2;\n",
    "\n",
    "    // Get the machine name.\n",
    "    MPI_Get_processor_name(machine_name, &name_len);\n",
    "\n",
    "    dx = 1.0f / (float)kTotalNumStep;\n",
    "    dx_2 = dx / 2.0f;\n",
    "    \n",
    "    for (size_t i = 0; i < num_step; i++)\n",
    "        x_pos_per_rank[i] = ((float)rank_num / (float)num_procs) + i * dx + dx_2;\n",
    "    \n",
    "    default_selector device_selector;\n",
    "\n",
    "    // exception handler\n",
    "    //\n",
    "    // The exception_list parameter is an iterable list of std::exception_ptr\n",
    "    // objects. But those pointers are not always directly readable. So, we\n",
    "    // rethrow the pointer, catch it,  and then we have the exception itself.\n",
    "    // Note: depending upon the operation there may be several exceptions.\n",
    "    //\n",
    "    auto exception_handler = [&](exception_list exceptionList) {\n",
    "        for (std::exception_ptr const& e : exceptionList) {\n",
    "            try {\n",
    "                std::rethrow_exception(e);\n",
    "            } catch (cl::sycl::exception const& e) {\n",
    "                std::cout << \"Failure\" << std::endl;\n",
    "                std::terminate();\n",
    "            }\n",
    "        }\n",
    "    };\n",
    "    \n",
    "    try {\n",
    "        // Create a device queue using DPC++ class queue\n",
    "        queue q(device_selector, exception_handler);\n",
    "        cout << \"Rank? \" << rank_num << \" of \" << num_procs\n",
    "            << \" runs on: \" << machine_name\n",
    "            << \", uses device: \" << q.get_device().get_info<info::device::name>()\n",
    "            << \"\\n\";\n",
    "        \n",
    "        // The size of amount of memory that will be given to the buffer.\n",
    "        range<1> num_items{kTotalNumStep / size_t(num_procs)};\n",
    "\n",
    "        // Buffers are used to tell SYCL which data will be shared between the host\n",
    "        // and the devices.\n",
    "        buffer<float, 1> x_pos_per_rank_buf(x_pos_per_rank, range<1>(kTotalNumStep / size_t(num_procs)));\n",
    "        buffer<float, 1> results_buf(results, range<1>(kTotalNumStep / size_t(num_procs)));\n",
    "\n",
    "        // Submit takes in a lambda that is passed in a command group handler\n",
    "        // constructed at runtime.\n",
    "        q.submit([&](handler& h) {\n",
    "            // Accessors are used to get access to the memory owned by the buffers.\n",
    "            auto x_pos_per_rank_accessor = x_pos_per_rank_buf.get_access<access::mode::read>(h);\n",
    "            auto results_accessor = results_buf.get_access<access::mode::write>(h);\n",
    "            \n",
    "            // Use parallel_for to calculate a partial of the number Pi in parallel.\n",
    "            // This creates a number of instances of kernel function.\n",
    "            h.parallel_for(num_items, [=](id<1> k) {\n",
    "                float x, dx;\n",
    "                dx = 1.0f / (float)kTotalNumStep;\n",
    "                x = x_pos_per_rank_accessor[k];\n",
    "                results_accessor[k] = (4.0f * dx) / (1.0f + x * x);\n",
    "            });\n",
    "        });\n",
    "    } catch (...) {\n",
    "        std::cout << \"Failure\" << std::endl;\n",
    "    }\n",
    "    \n",
    "    // Cleanup.\n",
    "    delete[] x_pos_per_rank;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Code\n",
    "Compilation of DPC++ code with MPI can be done using the __mpiicpc__ compiler command included with the Intel MPI Library. Simply set the compiler to be __dpcpp__.\n",
    "The script _compile_dpcpp.sh_ was created to easily submit compile comands on the DevCloud.\n",
    "The compile script compiles the newly written _pi_mpi_dpcpp.cpp_ with __mpiicpc__ using __dpcpp__.\n",
    "You may examine the launch script by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat compile_dpcpp.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will submit the execution of the compilation script using the __q__ script. The __q__ script submits jobs to the DevCloud and retrieves the output. The first arguments to __q__ is the script to execute. The second argument is the properties of the nodes to request. In the following cell, we're requesting one node with the property ppn=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 compile_dpcpp.sh; ./q compile_dpcpp.sh nodes=1:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Code\n",
    "Next we will execute the compiled binary with the __mpirun__ command to launch the MPI job using 4 processes. Examine the launch script by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat launch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to run the program on multiple nodes. In the following example, 2 nodes with GPUs are requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 launch.sh; ./q launch.sh nodes=2:gpu:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the execution completes, in the output, you should see the value of $\\pi$ as well as the nodes that each of the processes ran on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This simple exercise exposed you to the basics of using DPC++ and MPI with the [Intel® oneAPI Toolkits](https://software.intel.com/oneapi \"oneAPI main page\"). We encourage you to try these software toolkits on the [Intel® DevCloud](https://devcloud.intel.com/datacenter/connect) with your own designs.\n",
    "***\n",
    "\n",
    "@Intel Corporation | [\\*Trademark](https://www.intel.com/content/www/us/en/legal/trademarks.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
