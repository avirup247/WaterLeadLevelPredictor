{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MPI with OpenMP* Offload (C/C++)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objective\n",
    "\n",
    "* Explain how OpenMP can be used along with MPI on compute clusters with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Multi-Node Workflow with oneAPI\n",
    "With oneAPI, Accelerated code can be written in either a kernel (DPC++) or __directive based style__. Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA*__ to __Data Parallel C++__. Existing __Fortran__ programmers can leverage __OpenMP*__ for directive-based offload. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__ and existing __OpenCL*__ applications can remain in the OpenCL language or migrate to Data Parallel C++.\n",
    "\n",
    "To take advantage of multi-node clusters with GPUs, the aforementioned standards can be used in conjuction with MPI. The Intel® MPI Library is included with the Intel® oneAPI HPC toolkit.\n",
    "\n",
    "__Intel® Advisor__ is recommended to  __optimize__ the existing design for __vectorization and memory usage__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__ The __Intel® VTune™ Profiler__ can be used to profile your accelerated program while The __Intel® Trace Analyzer and Collector__ can be used to help you understand MPI application behavior.\n",
    "\n",
    "The figure below shows the recommended approach of different starting points for HPC developers.\n",
    "\n",
    "<img src=\"Assets/OneAPI_flow_mpi.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and OpenMP\n",
    "\n",
    "The following diagram illustrates how MPI and OpenMP interacts with the various nodes and accelerator devices in a cluster. Currently, the Intel® MPI Library supports host-based MPI where MPI calls are made from the hosts' CPUs. In the future, GPU-aware MPI will be supported so that GPU code will be able to directly interact with MPI.\n",
    "\n",
    "<img src=\"Assets/OpenMP_MPI.JPG\">\n",
    "\n",
    "In the current paradigm, the two standards serve different purposes and can be effectively used together. MPI is used to communicate between nodes, while OpenMP is used to accelerate computation on a single node using the CPUs or GPUs available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Pi\n",
    "\n",
    "To illustrate this concept, we're going to use both MPI and OpenMP to calculate pi through numeric integration.\n",
    "\n",
    "$$\n",
    "\\pi=\\int_0^1 \\frac{4}{1+x^2} dx\n",
    "$$\n",
    "\n",
    "The algorithm will use a discretization of the above equation.\n",
    "\n",
    "$$\n",
    "\\pi \\approx \\sum_{n=0}^{N-1} \\frac{4}{1+x_{i}^2} dx\n",
    "$$\n",
    "\n",
    "Where N=Number of steps  or iterations, dx = 1/N, and $x_{i}= (i+0.5)/N$\n",
    "\n",
    "In the code below, in _main()_, the iterations are divided based on the number of MPI ranks. Then for each rank, the group of iterations would execute in parallel on the GPU.\n",
    "\n",
    "Execute the cell below to write the code to file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/pi_mpi_omp.cpp\n",
    "\n",
    "//==============================================================\n",
    "// Copyright © 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "//\n",
    "// PI_MPI_OMP: Using OpenMP Offload in MPI program.\n",
    "//\n",
    "// Using OpenMP Offload, the code sample runs multiple MPI ranks to\n",
    "// distribute the calculation of the number Pi. Each rank offloads the\n",
    "// computation to an accelerator (GPU/CPU) using OpenMP Offload to compute\n",
    "// a partial compution of the number of Pi.\n",
    "//\n",
    "// For more information on the Intel(r) C++ Compiler or\n",
    "// Intel(r) MPI Library, visit the Intel(r) HPC Toolkit website.\n",
    "// https://software.intel.com/content/www/us/en/develop/tools/oneapi/hpc-toolkit.html\n",
    "//\n",
    "//******************************************************************************\n",
    "// Content: (version 1.0)\n",
    "//      Calculate the number Pi in parallel using its integral representation.\n",
    "//\n",
    "//******************************************************************************\n",
    "#include <mpi.h>\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "constexpr int kMaster = 0;\n",
    "constexpr long kIteration = 1024;\n",
    "constexpr long kScale = 45;\n",
    "constexpr long kTotalNumStep = kIteration * kScale;\n",
    "\n",
    "//******************************************************************************\n",
    "// Function description: computes the number Pi partially in parallel using OpenMP.\n",
    "// Each MPI rank calls this function to computes the number Pi partially.\n",
    "//******************************************************************************\n",
    "void CalculatePiParallel(float* results, int rank_num, int num_procs);\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "    int i, id, num_procs;\n",
    "    float total_pi;\n",
    "    MPI_Status stat;\n",
    "\n",
    "    // Start MPI.\n",
    "    if (MPI_Init(&argc, &argv) != MPI_SUCCESS) {\n",
    "        cout << \"Failed to initialize MPI\\n\";\n",
    "        exit(-1);\n",
    "    }\n",
    "    \n",
    "    // Create the communicator, and retrieve the number of processes.\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
    "    \n",
    "    // Determine the rank of the process.\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
    "\n",
    "    int num_step_per_rank = kTotalNumStep / num_procs;\n",
    "    float* results_per_rank = new float[num_step_per_rank];\n",
    "    for (size_t i = 0; i < num_step_per_rank; i++) results_per_rank[i] = 0.0;\n",
    "\n",
    "    // Calculate the Pi number partially in parallel.\n",
    "    CalculatePiParallel(results_per_rank, id, num_procs);\n",
    "\n",
    "    float sum = 0.0;\n",
    "    for (size_t i = 0; i < num_step_per_rank; i++) sum += results_per_rank[i];\n",
    "\n",
    "    delete[] results_per_rank;\n",
    "\n",
    "    MPI_Reduce(&sum, &total_pi, 1, MPI_FLOAT, MPI_SUM, kMaster, MPI_COMM_WORLD);\n",
    "\n",
    "    if (id == kMaster) cout << \"---> pi= \" << total_pi << \"\\n\";\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////\n",
    "//\n",
    "// Compute the number Pi partially on device: the partial result is\n",
    "// returned in \"results\".\n",
    "//\n",
    "////////////////////////////////////////////////////////////////////////\n",
    "void CalculatePiParallel(float* results, int rank_num, int num_procs) {\n",
    "    char machine_name[MPI_MAX_PROCESSOR_NAME];\n",
    "    int name_len;\n",
    "    int is_cpu=true;\n",
    "    int num_step = kTotalNumStep / num_procs;\n",
    "    float* x_pos_per_rank = new float[num_step];\n",
    "    float dx, dx_2;\n",
    "\n",
    "    // Get the machine name.\n",
    "    MPI_Get_processor_name(machine_name, &name_len);\n",
    "\n",
    "    dx = 1.0f / (float)kTotalNumStep;\n",
    "    dx_2 = dx / 2.0f;\n",
    "\n",
    "    for (size_t i = 0; i < num_step; i++)\n",
    "        x_pos_per_rank[i] = ((float)rank_num / (float)num_procs) + i * dx + dx_2;\n",
    "    \n",
    "    #pragma omp target map(from:is_cpu) map(to:x_pos_per_rank[0:num_step]) map(from:results[0:num_step])\n",
    "    {  \n",
    "        #pragma omp teams distribute parallel for simd\n",
    "        // Use loop to calculate a partial of the number Pi in parallel.\n",
    "        for (int k=0; k< num_step; k++) {\n",
    "            if (k==0) is_cpu=omp_is_initial_device();\n",
    "            float x = x_pos_per_rank[k];\n",
    "            results[k] = (4.0f * dx) / (1.0f + x * x);\n",
    "        }\n",
    "    }\n",
    "    cout << \"Rank \" << rank_num << \" of \" << num_procs\n",
    "         << \" runs on: \" << machine_name\n",
    "         << \", uses device: \" << (is_cpu?\"CPU\":\"GPU\")\n",
    "         << \"\\n\";\n",
    "\n",
    "    // Cleanup.\n",
    "    delete[] x_pos_per_rank;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Code\n",
    "Compilation of OpenMP offload code with MPI can be done using the __mpiicpc__ compiler command included with the Intel MPI Library. Simply set the compiler to be __icpx__ along with the options that enable OpenMP offload.\n",
    "The script _compile_omp_c.sh_ was created to easily submit compile comands on the DevCloud.\n",
    "The compile script compiles the newly written _pi_mpi_omp.cpp_ with __mpiicpc__ using __icpx__.\n",
    "You may examine the launch script by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat compile_omp_c.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will submit the execution of the compilation script using the __q__ script. The __q__ script submits jobs to the DevCloud and retrieves the output. The first arguments to __q__ is the script to execute. The second argument is the properties of the nodes to request. In the following cell, we're requesting one node with the property ppn=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 compile_omp_c.sh; ./q compile_omp_c.sh nodes=1:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Code\n",
    "Next we will execute the compiled binary with the __mpirun__ command to launch the MPI job using 4 processes. Examine the launch script by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat launch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to run the program on multiple nodes. In the following example, 2 nodes with GPUs are requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 launch.sh; ./q launch.sh nodes=2:gpu:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the execution completes, in the output, you should see the value of $\\pi$ as well as the nodes that each of the processes ran on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This simple exercise exposed you to the basics of using DPC++ and MPI with the [Intel® oneAPI Toolkits](https://software.intel.com/oneapi \"oneAPI main page\"). We encourage you to try these software toolkits on the [Intel® DevCloud](https://devcloud.intel.com/datacenter/connect) with your own designs.\n",
    "***\n",
    "\n",
    "@Intel Corporation | [\\*Trademark](https://www.intel.com/content/www/us/en/legal/trademarks.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
