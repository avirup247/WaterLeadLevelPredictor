{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MPI with OpenMP* Offload (Fortran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objective\n",
    "\n",
    "* Explain how OpenMP can be used along with MPI on compute clusters with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Multi-Node Workflow with oneAPI\n",
    "With oneAPI, Accelerated code can be written in either a kernel (DPC++) or __directive based style__. Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA*__ to __Data Parallel C++__. Existing __Fortran__ programmers can leverage __OpenMP*__ for directive-based offload. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__ and existing __OpenCL*__ applications can remain in the OpenCL language or migrate to Data Parallel C++.\n",
    "\n",
    "To take advantage of multi-node clusters with GPUs, the aforementioned standards can be used in conjuction with MPI. The Intel® MPI Library is included with the Intel® oneAPI HPC toolkit.\n",
    "\n",
    "__Intel® Advisor__ is recommended to  __optimize__ the existing design for __vectorization and memory usage__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__ The __Intel® VTune™ Profiler__ can be used to profile your accelerated program while The __Intel® Trace Analyzer and Collector__ can be used to help you understand MPI application behavior.\n",
    "\n",
    "The figure below shows the recommended approach of different starting points for HPC developers.\n",
    "\n",
    "<img src=\"Assets/OneAPI_flow_mpi.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and OpenMP\n",
    "\n",
    "The following diagram illustrates how MPI and OpenMP interacts with the various nodes and accelerator devices in a cluster. Currently, the Intel® MPI Library supports host-based MPI where MPI calls are made from the hosts' CPUs. In the future, GPU-aware MPI will be supported so that GPU code will be able to directly interact with MPI.\n",
    "\n",
    "<img src=\"Assets/OpenMP_MPI.JPG\">\n",
    "\n",
    "In the current paradigm, the two standards serve different purposes and can be effectively used together. MPI is used to communicate between nodes, while OpenMP is used to accelerate computation on a single node using the CPUs or GPUs available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Pi\n",
    "\n",
    "To illustrate this concept, we're going to use both MPI and OpenMP to calculate pi through numeric integration.\n",
    "\n",
    "$$\n",
    "\\pi=\\int_0^1 \\frac{4}{1+x^2} dx\n",
    "$$\n",
    "\n",
    "The algorithm will use a discretization of the above equation.\n",
    "\n",
    "$$\n",
    "\\pi \\approx \\sum_{n=0}^{N-1} \\frac{4}{1+x_{i}^2} dx\n",
    "$$\n",
    "\n",
    "Where N=Number of steps  or iterations, dx = 1/N, and $x_{i}= (i+0.5)/N$\n",
    "\n",
    "In the code below, in the program _main_, the iterations are divided based on the number of MPI ranks. Then for each rank, the group of iterations would execute in parallel on the GPU.\n",
    "\n",
    "Execute the cell below to write the code to file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/pi_mpi_omp.f90\n",
    "\n",
    "! ==============================================================\n",
    "! Copyright © 2020 Intel Corporation\n",
    "!\n",
    "! SPDX-License-Identifier: MIT\n",
    "! =============================================================\n",
    "!\n",
    "! PI_MPI_OMP: Using OpenMP Offload in MPI program.\n",
    "!\n",
    "! Using OpenMP Offload, the code sample runs multiple MPI ranks to\n",
    "! distribute the calculation of the number Pi. Each rank offloads the\n",
    "! computation to an accelerator (GPU/CPU) using OpenMP Offload to compute\n",
    "! a partial compution of the number of Pi.\n",
    "!\n",
    "! For more information on the Intel(r) Fortran Compiler or\n",
    "! Intel(r) MPI Library, visit the Intel(r) HPC Toolkit website.\n",
    "! https://software.intel.com/content/www/us/en/develop/tools/oneapi/hpc-toolkit.html\n",
    "!\n",
    "! ******************************************************************************\n",
    "! Content: (version 1.0)\n",
    "!      Calculate the number Pi in parallel using its integral representation.\n",
    "! ******************************************************************************\n",
    "! Function description: computes the number Pi partially in parallel using OpenMP.\n",
    "! Each MPI rank calls this function to computes the number Pi partially.\n",
    "!******************************************************************************\n",
    "PROGRAM main\n",
    "        use omp_lib\n",
    "        implicit none\n",
    "        include 'mpif.h'\n",
    "\n",
    "        integer :: id, num_procs, ierror, i\n",
    "        real :: total_pi, sum\n",
    "\n",
    "        integer, parameter :: kMaster=0, kIteration = 1024, kScale=25, kTotalNumStep = kIteration * kScale\n",
    "        integer :: num_step_per_rank\n",
    "        real, dimension (:), allocatable :: results_per_rank\n",
    "\n",
    "        ! Start MPI\n",
    "        call MPI_INIT(ierror)\n",
    "        !  Create the communicator, and retrieve the number of processes.\n",
    "        call MPI_COMM_SIZE(MPI_COMM_WORLD, num_procs, ierror)\n",
    "        ! Determine the rank of the process.\n",
    "        call MPI_COMM_RANK(MPI_COMM_WORLD, id, ierror)\n",
    "\n",
    "        num_step_per_rank = kTotalNumStep / num_procs\n",
    "        allocate (results_per_rank(num_step_per_rank))\n",
    "        do i=1, num_step_per_rank\n",
    "               results_per_rank(i) = 0.0\n",
    "        end do\n",
    "\n",
    "        ! Calculate the Pi number partially in parallel.\n",
    "        call CalculatePiParallel(results_per_rank, id, num_procs)\n",
    "\n",
    "        sum=0.0;\n",
    "        do i=1, num_step_per_rank\n",
    "                sum = sum + results_per_rank(i)\n",
    "        end do\n",
    "        \n",
    "        call MPI_REDUCE(sum, total_pi, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD, ierror)\n",
    "\n",
    "        if (id == 0) then\n",
    "                print *, \"---> pi= \", total_pi\n",
    "        endif\n",
    "\n",
    "        deallocate (results_per_rank)\n",
    "        call MPI_FINALIZE(ierror)\n",
    "\n",
    "CONTAINS\n",
    "SUBROUTINE CalculatePiParallel(results, rank_num, num_procs)\n",
    "        implicit none\n",
    "        integer :: num_step_per_rank, name_len, num_items, num_step, k\n",
    "        real, allocatable, dimension (:), intent(in out) :: results\n",
    "        real :: dx, dx_2, x\n",
    "        integer rank_num, num_procs\n",
    "        logical :: is_cpu = .true.\n",
    "        real, dimension (:), allocatable :: x_pos_per_rank\n",
    "        character*(MPI_MAX_PROCESSOR_NAME) machine_name\n",
    "\n",
    "        call MPI_GET_PROCESSOR_NAME(machine_name, name_len, ierror)\n",
    "\n",
    "        dx = 1.0 / kTotalNumStep\n",
    "        dx_2 = dx / 2.0\n",
    "\n",
    "        num_step = kTotalNumStep / num_procs;\n",
    "        allocate (x_pos_per_rank(num_step))\n",
    "\n",
    "        do i=1, num_step\n",
    "                x_pos_per_rank(i) = (real(rank_num) / real(num_procs)) + (i-1) * dx + dx_2\n",
    "        end do\n",
    "\n",
    "        ! Use loop to calculate a partial of the number Pi in parallel.\n",
    "        !$omp target teams distribute map(tofrom: is_cpu)\n",
    "        do k=1, num_step\n",
    "                if (k==1) then\n",
    "                        is_cpu=omp_is_initial_device()\n",
    "                end if\n",
    "                x = x_pos_per_rank(k)\n",
    "                results(k) = (4.0 * dx) / (1.0 + x * x);\n",
    "        end do\n",
    "\n",
    "        write (6, \"(A5, I2, A4, I1, A13, A14 )\", ADVANCE=\"NO\"), \"Rank \", rank_num, \"of \", num_procs, \" running on: \", machine_name\n",
    "\n",
    "        if (is_cpu) then\n",
    "                print *, \"using CPU\"\n",
    "        else\n",
    "                print *, \"using GPU\"\n",
    "        end if \n",
    "\n",
    "        deallocate (x_pos_per_rank)\n",
    "        return\n",
    "END SUBROUTINE\n",
    "END PROGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Code\n",
    "Compilation of OpenMP offload code with MPI can be done using the __mpiifort__ compiler command included with the Intel MPI Library. Simply set the compiler to be __ifx__ along with the options that enable OpenMP offload.\n",
    "The script _compile_omp_f.sh_ was created to easily submit compile comands on the DevCloud.\n",
    "The compile script compiles the newly written _pi_mpi_omp.f90_ with __mpiifort__ using __ifx__.\n",
    "You may examine the launch script by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat compile_omp_f.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will submit the execution of the compilation script using the __q__ script. The __q__ script submits jobs to the DevCloud and retrieves the output. The first arguments to __q__ is the script to execute. The second argument is the properties of the nodes to request. In the following cell, we're requesting one node with the property ppn=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 compile_omp_f.sh; ./q compile_omp_f.sh nodes=1:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Code\n",
    "Next we will execute the compiled binary with the __mpirun__ command to launch the MPI job using 4 processes. Examine the launch script by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat launch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to run the program on multiple nodes. In the following example, 2 nodes with GPUs are requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 launch.sh; ./q launch.sh nodes=2:gpu:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the execution completes, in the output, you should see the value of $\\pi$ as well as the nodes that each of the processes ran on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This simple exercise exposed you to the basics of using DPC++ and MPI with the [Intel® oneAPI Toolkits](https://software.intel.com/oneapi \"oneAPI main page\"). We encourage you to try these software toolkits on the [Intel® DevCloud](https://devcloud.intel.com/datacenter/connect) with your own designs.\n",
    "***\n",
    "\n",
    "@Intel Corporation | [\\*Trademark](https://www.intel.com/content/www/us/en/legal/trademarks.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
